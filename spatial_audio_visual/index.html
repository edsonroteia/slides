<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Spatial Understanding with AV-LLMs — Edson Araujo</title>
<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,300;0,6..72,400;0,6..72,500;0,6..72,600;0,6..72,700;1,6..72,400;1,6..72,500&family=Outfit:wght@300;400;500;600;700&family=IBM+Plex+Mono:wght@400;500;600&display=swap" rel="stylesheet">
<style>
*,*::before,*::after{margin:0;padding:0;box-sizing:border-box}

:root {
  --c-bg: #f6f4f0;
  --c-surface: #ffffff;
  --c-surface-dim: #edeae4;
  --c-text: #1a1a1e;
  --c-text-2: #4a4a52;
  --c-text-3: #7a7a84;
  --c-text-4: #a5a5ad;
  --c-border: #d8d5ce;
  --c-border-light: #e8e5de;
  --c-accent: #c44b28;
  --c-accent-light: #fdf0ec;
  --c-blue: #2663b0;
  --c-blue-light: #eaf0fa;
  --c-green: #1a7a4a;
  --c-green-light: #e8f5ee;
  --c-red: #b83232;
  --c-red-light: #fceaea;
  --c-purple: #6b42b8;
  --c-purple-light: #f2edfa;
  --c-yellow: #9a7a1a;
  --c-yellow-light: #fdf8e8;
  --f-head: 'Newsreader', 'Georgia', serif;
  --f-body: 'Outfit', system-ui, sans-serif;
  --f-mono: 'IBM Plex Mono', 'Menlo', monospace;
}

html { font-size: 16px; }
body {
  background: #2a2824;
  font-family: var(--f-body);
  color: var(--c-text);
  overflow: hidden;
  height: 100vh;
  width: 100vw;
  display: flex;
  align-items: center;
  justify-content: center;
}

/* ═══ DECK CONTAINER ═══ */
.deck {
  width: 1280px;
  height: 720px;
  position: relative;
  overflow: hidden;
  transform-origin: center center;
  flex-shrink: 0;
}

.slide {
  position: absolute;
  inset: 0;
  background: var(--c-bg);
  display: flex;
  flex-direction: column;
  opacity: 0;
  pointer-events: none;
  transition: opacity 0.45s ease;
  overflow: hidden;
}
.slide.active { opacity: 1; pointer-events: all; }

/* ═══ SLIDE CHROME ═══ */
.slide-inner {
  flex: 1;
  padding: 52px 64px 40px;
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

.slide-footer {
  height: 32px;
  padding: 0 64px;
  display: flex;
  align-items: center;
  justify-content: space-between;
  border-top: 1px solid var(--c-border-light);
  background: var(--c-surface);
  flex-shrink: 0;
}
.slide-footer span {
  font-family: var(--f-mono);
  font-size: 10px;
  color: var(--c-text-4);
  letter-spacing: 0.04em;
}

/* ═══ SECTION TAG ═══ */
.stag {
  font-family: var(--f-mono);
  font-size: 10px;
  font-weight: 500;
  letter-spacing: 0.12em;
  text-transform: uppercase;
  margin-bottom: 6px;
  display: inline-flex;
  align-items: center;
  gap: 8px;
}
.stag::before {
  content: '';
  width: 18px;
  height: 1.5px;
  display: inline-block;
}
.stag.orange { color: var(--c-accent); }
.stag.orange::before { background: var(--c-accent); }
.stag.blue { color: var(--c-blue); }
.stag.blue::before { background: var(--c-blue); }
.stag.green { color: var(--c-green); }
.stag.green::before { background: var(--c-green); }
.stag.purple { color: var(--c-purple); }
.stag.purple::before { background: var(--c-purple); }
.stag.dim { color: var(--c-text-3); }
.stag.dim::before { background: var(--c-text-3); }

/* ═══ TYPOGRAPHY ═══ */
.sh { /* slide heading */
  font-family: var(--f-head);
  font-weight: 500;
  line-height: 1.15;
  color: var(--c-text);
  letter-spacing: -0.015em;
}
.sh-xl { font-size: 42px; margin-bottom: 20px; }
.sh-lg { font-size: 32px; margin-bottom: 16px; }
.sh-md { font-size: 22px; margin-bottom: 10px; }
.sh-sm { font-size: 17px; margin-bottom: 6px; }

.sh em {
  font-style: italic;
  color: var(--c-accent);
  font-weight: 500;
}

.sp { /* slide paragraph */
  font-size: 14px;
  line-height: 1.6;
  color: var(--c-text-2);
  max-width: 680px;
}
.sp + .sp { margin-top: 6px; }
.sp strong { color: var(--c-text); font-weight: 600; }

.smono {
  font-family: var(--f-mono);
  font-size: 12px;
}

/* ═══ LAYOUT HELPERS ═══ */
.row { display: flex; gap: 24px; }
.row-tight { display: flex; gap: 12px; }
.col { display: flex; flex-direction: column; }
.col-2 { display: grid; grid-template-columns: 1fr 1fr; gap: 24px; }
.col-3 { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 16px; }
.col-5-7 { display: grid; grid-template-columns: 5fr 7fr; gap: 28px; }
.col-7-5 { display: grid; grid-template-columns: 7fr 5fr; gap: 28px; }
.col-4-8 { display: grid; grid-template-columns: 4fr 8fr; gap: 28px; }
.flex-1 { flex: 1; min-height: 0; }
.gap-8 { gap: 8px; }
.gap-12 { gap: 12px; }
.gap-16 { gap: 16px; }
.gap-20 { gap: 20px; }
.mt-auto { margin-top: auto; }
.mt-8 { margin-top: 8px; }
.mt-12 { margin-top: 12px; }
.mt-16 { margin-top: 16px; }
.mb-8 { margin-bottom: 8px; }
.mb-12 { margin-bottom: 12px; }
.mb-16 { margin-bottom: 16px; }

/* ═══ CARDS ═══ */
.card {
  background: var(--c-surface);
  border: 1px solid var(--c-border-light);
  border-radius: 6px;
  padding: 16px 18px;
}
.card.flush { padding: 0; overflow: hidden; }
.card.accent { border-left: 2px solid var(--c-accent); }
.card.cblue { border-left: 2px solid var(--c-blue); }
.card.cgreen { border-left: 2px solid var(--c-green); }
.card.cred { border-left: 2px solid var(--c-red); }
.card.cpurple { border-left: 2px solid var(--c-purple); }
.card.soft { border-left: none; }

.card-title {
  font-family: var(--f-body);
  font-size: 13px;
  font-weight: 600;
  color: var(--c-text);
  margin-bottom: 4px;
}

.card p {
  font-size: 12.5px;
  line-height: 1.55;
  color: var(--c-text-2);
}
.card p strong { color: var(--c-text); }

/* ═══ CALLOUT ═══ */
.callout {
  border-radius: 5px;
  padding: 10px 14px;
  font-size: 12.5px;
  line-height: 1.5;
  color: var(--c-text);
}
.callout.orange { background: var(--c-accent-light); border: 1px solid rgba(196,75,40,0.10); }
.callout.blue { background: var(--c-blue-light); border: 1px solid rgba(38,99,176,0.10); }
.callout.green { background: var(--c-green-light); border: 1px solid rgba(26,122,74,0.10); }
.callout.red { background: var(--c-red-light); border: 1px solid rgba(184,50,50,0.10); }
.callout.yellow { background: var(--c-yellow-light); border: 1px solid rgba(154,122,26,0.10); }
.callout strong { font-weight: 600; }

/* ═══ STAT BLOCKS ═══ */
.stats { display: flex; gap: 10px; flex-wrap: wrap; }
.stat {
  background: var(--c-surface);
  border: 1px solid var(--c-border-light);
  border-radius: 5px;
  padding: 10px 16px;
  text-align: center;
  min-width: 90px;
}
.stat-val {
  font-family: var(--f-mono);
  font-size: 22px;
  font-weight: 600;
  line-height: 1;
  margin-bottom: 3px;
}
.stat-val.green { color: var(--c-green); }
.stat-val.red { color: var(--c-red); }
.stat-val.orange { color: var(--c-accent); }
.stat-val.blue { color: var(--c-blue); }
.stat-lbl {
  font-size: 10px;
  color: var(--c-text-3);
  font-weight: 500;
  letter-spacing: 0.02em;
}

/* ═══ TABLE ═══ */
.tbl {
  width: 100%;
  border-collapse: collapse;
  font-size: 12.5px;
}
.tbl th {
  font-family: var(--f-mono);
  font-size: 9.5px;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--c-text-3);
  text-align: left;
  padding: 7px 10px;
  border-bottom: 2px solid var(--c-border);
  white-space: nowrap;
}
.tbl td {
  padding: 6px 10px;
  border-bottom: 1px solid var(--c-border-light);
  color: var(--c-text-2);
  font-variant-numeric: tabular-nums;
}
.tbl tr.hl td { background: var(--c-accent-light); color: var(--c-text); font-weight: 600; }
.tbl tr.hlb td { background: var(--c-blue-light); color: var(--c-text); font-weight: 600; }
.tbl tr.dim td { color: var(--c-text-4); }
.tbl .best { color: var(--c-green); font-weight: 600; }
.tbl .bad { color: var(--c-red); }
.tbl .mid { color: var(--c-accent); }

/* ═══ IMAGES ═══ */
.chart-frame {
  background: var(--c-surface);
  border: 1px solid var(--c-border-light);
  border-radius: 6px;
  padding: 8px;
  display: flex;
  align-items: center;
  justify-content: center;
  overflow: hidden;
}
.chart-frame img,
.chart-frame video {
  width: 100%;
  height: 100%;
  object-fit: contain;
  border-radius: 3px;
}

/* ═══ TAGS ═══ */
.tag {
  display: inline-block;
  font-family: var(--f-mono);
  font-size: 9px;
  font-weight: 500;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  padding: 3px 8px;
  border-radius: 3px;
}
.tag.orange { background: var(--c-accent-light); color: var(--c-accent); }
.tag.blue { background: var(--c-blue-light); color: var(--c-blue); }
.tag.green { background: var(--c-green-light); color: var(--c-green); }
.tag.red { background: var(--c-red-light); color: var(--c-red); }
.tag.purple { background: var(--c-purple-light); color: var(--c-purple); }

/* ═══ BAR CHARTS (CSS) ═══ */
.bar-row {
  display: flex;
  align-items: center;
  gap: 8px;
  margin-bottom: 6px;
}
.bar-label {
  font-size: 11px;
  color: var(--c-text-3);
  width: 100px;
  text-align: right;
  flex-shrink: 0;
  font-weight: 500;
}
.bar-track {
  flex: 1;
  height: 20px;
  background: var(--c-surface-dim);
  border-radius: 3px;
  overflow: hidden;
  position: relative;
}
.bar-fill {
  height: 100%;
  border-radius: 3px;
  display: flex;
  align-items: center;
  padding-left: 8px;
  font-family: var(--f-mono);
  font-size: 10px;
  font-weight: 600;
  color: white;
  white-space: nowrap;
  transition: width 0.6s ease;
}

/* ═══ PIPELINE STEP ═══ */
.pstep {
  display: flex;
  gap: 10px;
  align-items: flex-start;
}
.pstep + .pstep { margin-top: 10px; }
.pstep-num {
  font-family: var(--f-mono);
  font-size: 10px;
  font-weight: 700;
  width: 22px;
  height: 22px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  margin-top: 1px;
  color: white;
}
.pstep-num.orange { background: var(--c-accent); }
.pstep-num.blue { background: var(--c-blue); }
.pstep-num.green { background: var(--c-green); }
.pstep-num.purple { background: var(--c-purple); }
.pstep-body h4 {
  font-family: var(--f-body);
  font-size: 13px;
  font-weight: 600;
  color: var(--c-text);
  margin-bottom: 2px;
}
.pstep-body p {
  font-size: 11.5px;
  line-height: 1.5;
  color: var(--c-text-2);
}

/* ═══ MONO CODE BLOCK ═══ */
.codeblock {
  font-family: var(--f-mono);
  font-size: 11px;
  line-height: 1.75;
  background: #1e1e24;
  color: #c8c8d0;
  border-radius: 6px;
  padding: 16px 20px;
  overflow: auto;
}
.codeblock .cm { color: #6a6a78; }
.codeblock .ok { color: #4aba7a; }
.codeblock .err { color: #e05050; }
.codeblock .warn { color: #d89030; }
.codeblock .kw { color: #e0c080; font-weight: 600; }

/* ═══ INSIGHT MARKER ═══ */
.insight-row {
  display: flex;
  gap: 10px;
  align-items: flex-start;
}
.insight-tag {
  font-family: var(--f-mono);
  font-size: 9px;
  font-weight: 600;
  letter-spacing: 0.08em;
  padding: 3px 8px;
  border-radius: 3px;
  white-space: nowrap;
  flex-shrink: 0;
  margin-top: 2px;
}
.insight-tag.orange { background: var(--c-accent-light); color: var(--c-accent); }
.insight-tag.blue { background: var(--c-blue-light); color: var(--c-blue); }
.insight-tag.red { background: var(--c-red-light); color: var(--c-red); }
.insight-row p {
  font-size: 12.5px;
  line-height: 1.5;
  color: var(--c-text-2);
}
.insight-row p strong { color: var(--c-text); }

/* ═══ TITLE SLIDE ═══ */
.title-layout {
  flex: 1;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  text-align: center;
  padding: 60px;
}
.title-layout .sh { max-width: 100%; }
.title-rule {
  width: 48px;
  height: 2px;
  background: var(--c-accent);
  margin: 20px auto 16px;
  border-radius: 1px;
}
.title-author {
  font-family: var(--f-body);
  font-size: 16px;
  font-weight: 400;
  color: var(--c-text-2);
}
.title-sub {
  font-size: 13px;
  color: var(--c-text-3);
  margin-top: 4px;
}

/* ═══ SECTION DIVIDER ═══ */
.divider-layout {
  flex: 1;
  display: flex;
  flex-direction: column;
  justify-content: center;
  padding: 60px 80px;
}
.divider-num {
  font-family: var(--f-head);
  font-size: 72px;
  font-weight: 300;
  color: var(--c-border);
  line-height: 1;
  margin-bottom: 8px;
}
.divider-layout .sh { max-width: 70%; }
.divider-layout .sp { margin-top: 8px; }

/* ═══ PROGRESS BAR ═══ */
.progress-bar {
  position: absolute;
  bottom: 32px;
  left: 0;
  height: 2px;
  background: var(--c-accent);
  transition: width 0.4s ease;
  z-index: 10;
}

/* ═══ SLIDE JUMP DIALOG ═══ */
#slide-jump {
  position: fixed; top: 0; left: 0; right: 0; bottom: 0;
  display: none; align-items: center; justify-content: center;
  background: rgba(0,0,0,0.30); z-index: 9999;
}
#slide-jump.active { display: flex; }
#slide-jump-box {
  background: #fff; border-radius: 12px; padding: 20px 28px;
  box-shadow: 0 8px 32px rgba(0,0,0,0.18);
  font-family: var(--f-body); text-align: center;
}
#slide-jump-box label { font-size: 14px; color: var(--c-text-2); display: block; margin-bottom: 8px; }
#slide-jump-box input {
  width: 80px; font-size: 28px; text-align: center; border: 2px solid #ddd;
  border-radius: 8px; padding: 6px; outline: none; font-family: var(--f-mono);
}
#slide-jump-box input:focus { border-color: var(--c-accent); }
#slide-jump-box .hint { font-size: 11px; color: #999; margin-top: 6px; }

/* ═══ INTRO SLIDE — PAPER HIGHLIGHT ═══ */
.intro-papers .card {
  transition: opacity 0.35s ease, transform 0.35s ease, box-shadow 0.35s ease;
}
.intro-papers.highlighting .card {
  opacity: 0.30;
  transform: scale(0.98);
}
.intro-papers.highlighting .card.spot {
  opacity: 1;
  transform: scale(1.01);
  box-shadow: 0 2px 12px rgba(0,0,0,0.08);
  z-index: 2;
  position: relative;
}

/* ═══ GENERIC SUB-STEP HIGHLIGHTING ═══ */
[data-step] {
  transition: opacity 0.35s ease, transform 0.35s ease;
}
.step-active [data-step] {
  opacity: 0.30;
  transform: scale(0.98);
}
.step-active [data-step].step-lit {
  opacity: 1;
  transform: scale(1);
}

/* ═══ SOTA TABLE — ROW HIGHLIGHTING ═══ */
.sota-tbl tbody tr { transition: opacity 0.35s ease; }
.sota-tbl.highlighting tbody tr { opacity: 0.35; }
.sota-tbl.highlighting tbody tr.sota-lit { opacity: 1; }
.sota-tbl.highlighting tbody tr.sota-lit td { background: var(--c-yellow-light); }
.sota-tbl.highlighting tbody tr.sota-lit.hl td { background: var(--c-accent-light); }
.sota-tbl.highlighting tbody tr.sota-lit.hlb td { background: var(--c-blue-light); }
.sota-caption [data-step] { display: none; opacity: 1 !important; transform: none !important; }
.sota-caption [data-step].step-lit { display: block; }

/* ═══ INTRO SLIDE — BOTTOM BANNER ═══ */
.intro-banner {
  display: flex;
  flex-direction: column;
  gap: 5px;
  margin-top: auto;
}
.intro-banner-item {
  display: flex;
  align-items: center;
  gap: 10px;
  font-size: 11.5px;
  color: var(--c-text-2);
  line-height: 1.45;
  transition: opacity 0.35s ease, transform 0.35s ease;
}
.intro-papers.highlighting ~ .intro-banner .intro-banner-item {
  opacity: 0.30;
  transform: scale(0.98);
}
.intro-papers.highlighting ~ .intro-banner .intro-banner-item.spot {
  opacity: 1;
  transform: scale(1.0);
}
.intro-banner-item .elliot-logo {
  display: inline-flex;
  align-items: center;
  background: #1e1e24;
  border-radius: 4px;
  padding: 4px 8px;
  flex-shrink: 0;
}
.intro-banner-item .elliot-logo img {
  height: 14px;
}
.intro-banner-item a {
  color: var(--c-blue);
  text-decoration: none;
  font-weight: 500;
}
.intro-banner-item a:hover { text-decoration: underline; }

/* ═══ RESPONSIVE SCALING (handled by JS) ═══ */
/* ═══ VIDEO THUMB + OVERLAY ═══ */
.video-thumb { position: relative; }
.video-thumb .play-icon {
  position: absolute; inset: 0; display: flex; align-items: center; justify-content: center;
  font-size: 32px; color: #fff; background: rgba(0,0,0,0.25); border-radius: 6px;
  opacity: 0; transition: opacity 0.2s; pointer-events: none;
}
.video-thumb:hover .play-icon { opacity: 1; }
#video-overlay, #img-overlay {
  display: none; position: fixed; inset: 0; z-index: 9999;
  background: rgba(0,0,0,0.85); align-items: center; justify-content: center; cursor: pointer;
}
#video-overlay.active, #img-overlay.active { display: flex; }
#video-overlay video {
  max-width: 90vw; max-height: 90vh; border-radius: 8px; outline: none;
}
#img-overlay img {
  max-width: 90vw; max-height: 90vh; border-radius: 8px; cursor: default;
}
.chart-frame img { cursor: zoom-in; }

/* ═══ MAP CAROUSEL ═══ */
.map-carousel { position: relative; }
.map-carousel .carousel-track { position: relative; overflow: hidden; border-radius: 3px; }
.map-carousel .carousel-track img { display: none; max-height: 260px; width: auto; margin: 0 auto; border-radius: 3px; cursor: zoom-in; }
.map-carousel .carousel-track img.active { display: block; }
.map-carousel .carousel-nav {
  display: flex; align-items: center; justify-content: center; gap: 8px; margin-top: 6px;
}
.map-carousel .carousel-btn {
  background: var(--c-surface-dim); border: 1px solid var(--c-border); border-radius: 4px;
  width: 24px; height: 24px; cursor: pointer; font-size: 12px; line-height: 1;
  display: flex; align-items: center; justify-content: center; color: var(--c-text-2);
  transition: background 0.15s;
}
.map-carousel .carousel-btn:hover { background: var(--c-border-light); }
.map-carousel .carousel-dots { display: flex; gap: 5px; }
.map-carousel .carousel-dot {
  width: 7px; height: 7px; border-radius: 50%; background: var(--c-border);
  cursor: pointer; transition: background 0.15s;
}
.map-carousel .carousel-dot.active { background: var(--c-green); }
.map-carousel .carousel-label {
  font-size: 9px; font-family: var(--f-mono); color: var(--c-text-4); min-width: 120px; text-align: center;
}

/* ═══ JSON PREVIEW ═══ */
.json-preview {
  background: #1e1e2e; color: #cdd6f4; border-radius: 5px; padding: 8px 10px;
  font-family: var(--f-mono); font-size: 9px; line-height: 1.5;
  max-height: 130px; overflow-y: auto; white-space: pre;
}
.json-preview .jk { color: #89b4fa; }
.json-preview .js { color: #a6e3a1; }
.json-preview .jn { color: #fab387; }
.json-preview .jp { color: #9399b2; }
</style>
</head>
<body>

<div class="deck" id="deck">

<!-- ════════════════════ SLIDE 1: TITLE ════════════════════ -->
<div class="slide active" data-section="">
  <div class="title-layout">
    <div class="sh sh-xl" style="font-size:48px; max-width:900px;">
      Spatial Understanding<br>with <em>Audio-Visual LLMs</em>
    </div>
    <div class="title-sub">Failure Mode Analysis &amp; Research Directions for Spatial Reasoning</div>
    <div class="title-rule"></div>
    <div class="title-author">Edson Araujo & Prof. Hilde Kuehne</div>

  </div>
  <div class="slide-footer">
    <span>Spatial Understanding with AV-LLMs</span>
    <span id="counter">1 / 18</span>
  </div>
  <div class="progress-bar" id="progress"></div>
</div>

<!-- ════════════════════ SLIDE 2: ABOUT ME ════════════════════ -->
<div class="slide" data-section="" data-steps="5" id="slide-intro">
  <div class="slide-inner">
    <div class="stag dim">About Me</div>
    <div class="sh sh-lg">Quick Introduction</div>

    <div style="display:flex; gap:36px; flex:1; align-items:flex-start; margin-top:8px;">
      <!-- LEFT: Photo + affiliation -->
      <div style="display:flex; flex-direction:column; align-items:center; flex-shrink:0; width:180px;">
        <img src="assets/profile_pic.jpeg" alt="Edson Araujo" style="width:140px; height:140px; border-radius:50%; object-fit:cover; border:3px solid var(--c-border-light); box-shadow:0 4px 16px rgba(0,0,0,0.08);">
        <div style="margin-top:14px; text-align:center;">
          <div style="font-size:16px; font-weight:600; color:var(--c-text); font-family:var(--f-body);">Edson Araujo</div>
          <div style="font-size:11px; color:var(--c-text-3); margin-top:3px; line-height:1.45;">3rd-year PhD Student<br>University of Tübingen<br>Advisor: Prof. Hilde Kuehne</div>
          <div style="font-size:9.5px; color:var(--c-text-4); margin-top:6px; font-family:var(--f-mono); letter-spacing:0.02em;">MIT-IBM Watson AI<br>Sight &amp; Sound Project</div>
        </div>
      </div>

      <!-- RIGHT: Research + Papers -->
      <div style="flex:1; display:flex; flex-direction:column; gap:14px;">
        <div style="font-size:13px; color:var(--c-text-2); line-height:1.6; max-width:720px;">
          Research focus: <strong style="color:var(--c-text);">audio-visual reasoning</strong>, <strong style="color:var(--c-text);">multimodal LLMs</strong>, self-supervised learning, and test-time adaptation.
        </div>

        <div class="intro-papers" style="display:grid; grid-template-columns:1fr 1fr; gap:10px;">
          <div class="card accent" data-paper="0" style="padding:12px 14px;">
            <div style="display:flex; align-items:center; gap:6px; margin-bottom:4px;">
              <span class="tag orange" style="font-size:8px;">CVPR 2025</span>
            </div>
            <div style="font-size:13px; font-weight:600; color:var(--c-text); margin-bottom:3px;">CAV-MAE Sync</div>
            <p style="font-size:11px; line-height:1.5; color:var(--c-text-2);">Fine-grained audio-visual alignment via <strong>temporal sequences</strong> instead of global representations.</p>
          </div>

          <div class="card cblue" data-paper="2" style="padding:12px 14px;">
            <div style="display:flex; align-items:center; gap:6px; margin-bottom:4px;">
              <span class="tag blue" style="font-size:8px;">Under Review</span>
            </div>
            <div style="font-size:13px; font-weight:600; color:var(--c-text); margin-bottom:3px;">AVRT</div>
            <p style="font-size:11px; line-height:1.5; color:var(--c-text-2);">Audio-Visual Reasoning Transfer, generates high-quality AV reasoning traces from <strong>single-modality teachers</strong>.</p>
          </div>

          <div class="card cgreen" data-paper="1" style="padding:12px 14px;">
            <div style="display:flex; align-items:center; gap:6px; margin-bottom:4px;">
              <span class="tag green" style="font-size:8px;">ASRU 2025</span>
            </div>
            <div style="font-size:13px; font-weight:600; color:var(--c-text); margin-bottom:3px;">Omni-R1</div>
            <p style="font-size:11px; line-height:1.5; color:var(--c-text-2);">Fine-tunes Qwen2.5-Omni with <strong>GRPO</strong>, achieving SOTA on the MMAU benchmark.</p>
          </div>

          <div class="card cpurple" data-paper="3" style="padding:12px 14px;">
            <div style="display:flex; align-items:center; gap:6px; margin-bottom:4px;">
              <span class="tag purple" style="font-size:8px;">Under Review</span>
            </div>
            <div style="font-size:13px; font-weight:600; color:var(--c-text); margin-bottom:3px;">TTA-Vid</div>
            <p style="font-size:11px; line-height:1.5; color:var(--c-text-2);">Test-time adaptation for videos using <strong>RL and a multi-armed bandit-based frame selection.</strong></p>
          </div>
        </div>

        <div class="intro-banner">
          <div class="intro-banner-item">
            <span class="elliot-logo"><img src="assets/logo-elliot.svg" alt="ELLIOT"></span>
            <span>Member of the <a href="https://www.elliot-ai.eu/" target="_blank"><strong>ELLIOT</strong></a> project, running large-scale experiments with VLMs.</span>
          </div>
          <div class="intro-banner-item">
            <span style="font-size:14px; line-height:1;">&#127891;</span>
            <span>Organizing the <a href="https://mmfm-workshop.github.io" target="_blank"><strong>MMFM Workshop</strong> @ CVPR 2026</a>: What is Next in Multimodal Foundation Models?</span>
          </div>
          <div class="intro-banner-item">
            <span style="font-size:14px; line-height:1;">&#127760;</span>
            <span>Currently exploring <strong style="color:var(--c-accent);">spatial understanding of AV-LLMs</strong></span>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>Edson Araujo · edsonroteia.github.io</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 3: SECTION — CONTEXT ════════════════════ -->
<div class="slide" data-section="Context">
  <div class="divider-layout">
    <div class="divider-num">01</div>
    <div class="sh sh-xl">Problem &amp; Context</div>
    <p class="sp" style="max-width:560px;">Spatial reasoning in dynamic audio-visual environments</p>
  </div>
  <div class="slide-footer"><span>01 — Context</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 3a: BENCHMARK OVERVIEW ════════════════════ -->
<div class="slide" data-section="Context">
  <div class="slide-inner">
    <div class="stag orange">The Benchmark</div>
    <div class="sh sh-lg">SAVVY-Bench <em>(NeurIPS 2025 - Oral)</em></div>
    <p class="sp" style="margin-bottom:16px;">Spatial Audio-Visual Question Answering — 4 tasks testing whether models can localize sounds and objects in 3D space from egocentric video with audio.</p>

    <div style="flex:1; display:flex; align-items:center; justify-content:center; min-height:0;">
      <div class="chart-frame" style="padding:2px; display:inline-block; line-height:0;">
        <img src="assets/savvy_paper_figs/savvybench_image_example.png" alt="Benchmark example showing 3D scene with ego/allo QA tasks" style="max-height:100%; max-width:100%; height:auto; width:auto; object-fit:contain; border-radius:3px;">
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>01 — Context</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 3b: TASK TYPES + DATA SOURCE ════════════════════ -->
<div class="slide" data-section="Context" data-steps="1">
  <div class="slide-inner">
    <div class="stag orange">Task Breakdown</div>
    <div class="sh sh-lg">4 Spatial QA Tasks</div>

    <div class="col gap-8 flex-1" style="margin-top:8px;">
      <!-- 4 Task Types — horizontal row -->
      <div data-step="0" style="display:grid; grid-template-columns:repeat(4, 1fr); gap:10px;">
        <div class="card accent" style="padding:16px 18px;">
          <div style="display:flex; align-items:center; gap:7px; margin-bottom:8px;">
            <span class="tag orange" style="font-size:9.5px;">Egocentric</span>
            <span style="font-size:15px; font-weight:600; color:var(--c-text);">Direction</span>
          </div>
          <p style="font-size:13.5px; line-height:1.55; color:var(--c-text-2);">"Relative to where you are facing, where is the other person?"</p>
          <p style="font-size:12.5px; margin-top:6px; color:var(--c-text-3); font-weight:600;">→ front-left / front-right / back-left / back-right</p>
        </div>
        <div class="card accent" style="padding:16px 18px;">
          <div style="display:flex; align-items:center; gap:7px; margin-bottom:8px;">
            <span class="tag orange" style="font-size:9.5px;">Egocentric</span>
            <span style="font-size:15px; font-weight:600; color:var(--c-text);">Distance</span>
          </div>
          <p style="font-size:13.5px; line-height:1.55; color:var(--c-text-2);">"What is the distance between you and the other person?"</p>
          <p style="font-size:12.5px; margin-top:6px; color:var(--c-text-3); font-weight:600;">→ meters (continuous)</p>
        </div>
        <div class="card cblue" style="padding:16px 18px;">
          <div style="display:flex; align-items:center; gap:7px; margin-bottom:8px;">
            <span class="tag blue" style="font-size:9.5px;">Allocentric</span>
            <span style="font-size:15px; font-weight:600; color:var(--c-text);">Direction</span>
          </div>
          <p style="font-size:13.5px; line-height:1.55; color:var(--c-text-2);">"Standing by the cabinet and facing the TV, where is the speaker?"</p>
          <p style="font-size:12.5px; margin-top:6px; color:var(--c-text-3); font-weight:600;">→ left / right / front / back</p>
        </div>
        <div class="card cblue" style="padding:16px 18px;">
          <div style="display:flex; align-items:center; gap:7px; margin-bottom:8px;">
            <span class="tag blue" style="font-size:9.5px;">Allocentric</span>
            <span style="font-size:15px; font-weight:600; color:var(--c-text);">Distance</span>
          </div>
          <p style="font-size:13.5px; line-height:1.55; color:var(--c-text-2);">"What is the distance between the table and the sound source?"</p>
          <p style="font-size:12.5px; margin-top:6px; color:var(--c-text-3); font-weight:600;">→ meters (continuous)</p>
        </div>
      </div>
      <!-- Video examples -->
      <div data-step="1" class="col-2" style="gap:24px;">
        <div class="col gap-2" style="align-items:center;">
          <div class="chart-frame video-thumb" style="max-height:340px; cursor:pointer;" onclick="openVideoOverlay('assets/savvybench_examples/video_ego_dir.mp4')">
            <video src="assets/savvybench_examples/video_ego_dir.mp4" autoplay muted loop playsinline></video>
            <div class="play-icon">&#9654;</div>
          </div>
          <div style="text-align:center;">
            <span class="tag orange" style="font-size:8px;">Egocentric Direction</span>
            <p style="font-size:9.5px; color:var(--c-text-2); margin-top:2px; line-height:1.3;">Camera wearer's perspective: "where is the other person relative to where <strong>you</strong> are facing?"</p>
          </div>
        </div>
        <div class="col gap-2" style="align-items:center;">
          <div class="chart-frame video-thumb" style="max-height:340px; cursor:pointer;" onclick="openVideoOverlay('assets/savvybench_examples/video_exo_dist.mp4')">
            <video src="assets/savvybench_examples/video_exo_dist.mp4" autoplay muted loop playsinline></video>
            <div class="play-icon">&#9654;</div>
          </div>
          <div style="text-align:center;">
            <span class="tag blue" style="font-size:8px;">Allocentric Distance</span>
            <p style="font-size:9.5px; color:var(--c-text-2); margin-top:2px; line-height:1.3;">Third-party reference frame: "what is the distance between the <strong>table</strong> and the sound source?"</p>
          </div>
        </div>
      </div>

      <!-- Bottom: Data source + stats
      <div style="display:flex; align-items:center; gap:20px; font-size:12px; color:var(--c-text-2); line-height:1.5; padding:10px 14px; background:var(--c-surface-dim); border-radius:6px;">
        <div style="display:flex; gap:16px; flex-shrink:0;">
          <div style="text-align:center;"><span style="font-size:18px; font-weight:700; color:var(--c-accent); display:block;">1,243</span><span style="font-size:10px; color:var(--c-text-3);">QA Pairs</span></div>
          <div style="text-align:center;"><span style="font-size:18px; font-weight:700; color:var(--c-green); display:block;">7-ch</span><span style="font-size:10px; color:var(--c-text-3);">Spatial Audio</span></div>
        </div>
        <div style="width:1px; height:28px; background:var(--c-border-light); flex-shrink:0;"></div>
        <div><strong style="color:var(--c-text);">Data source: Aria Everyday Activities (AEA)</strong> — egocentric recordings from Project Aria glasses with 7-channel spatial audio, IMU, eye tracking, and <strong>3D ground-truth trajectories</strong>.</div>
      </div> -->
    </div> 
  </div>
  <div class="slide-footer"><span>01 — Context</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 3d: REQUIRED CAPABILITIES ════════════════════ -->
<div class="slide" data-section="Context" data-steps="2">
  <div class="slide-inner">
    <div class="stag orange">Required Capabilities</div>
    <div class="sh sh-lg">What Must a Model Do to <em>Answer Correctly?</em></div>

    <div class="col gap-16 flex-1" style="margin-top:12px;">

      <!-- Capability 1: Temporal Grounding -->
      <div data-step="0" class="card accent" style="padding:14px 18px;">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
          <div class="pstep-num orange">1</div>
          <span style="font-size:14px; font-weight:600; color:var(--c-text);">Audio-Based Temporal Grounding</span>
          <span style="font-size:11px; color:var(--c-text-3); margin-left:auto;">Match described speech/sound → timestamp</span>
        </div>
        <div style="background:var(--c-surface-dim); border-radius:5px; padding:10px 14px; font-size:12.5px; line-height:1.6; color:var(--c-text-2);">
          <span style="font-family:var(--f-mono); font-size:10px; font-weight:600; color:var(--c-accent); letter-spacing:0.04em;">EXAMPLE</span><br>
          <strong style="color:var(--c-text);">Q:</strong> Imagine you are the camera wearer, when <span style="background:rgba(196,75,40,0.15); padding:1px 4px; border-radius:3px; color:var(--c-accent); font-weight:600;">the speech topic "identify the artist as phoenix local"</span> comes up, relative to where you are facing, where is the other person?
        </div>
      </div>

      <!-- Capability 2: Spatial Localization -->
      <div data-step="1" class="card cblue" style="padding:14px 18px;">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
          <div class="pstep-num blue">2</div>
          <span style="font-size:14px; font-weight:600; color:var(--c-text);">Spatial Localization (Self + Target)</span>
          <span style="font-size:11px; color:var(--c-text-3); margin-left:auto;">Camera pose, orientation, object position in 3D</span>
        </div>
        <div style="background:var(--c-surface-dim); border-radius:5px; padding:10px 14px; font-size:12.5px; line-height:1.6; color:var(--c-text-2);">
          <span style="font-family:var(--f-mono); font-size:10px; font-weight:600; color:var(--c-blue); letter-spacing:0.04em;">EXAMPLE</span><br>
          <strong style="color:var(--c-text);">Q:</strong> …<span style="background:rgba(38,99,176,0.15); padding:1px 4px; border-radius:3px; color:var(--c-blue); font-weight:600;">relative to where you are facing</span>, where is the other person: front-left, front-right, back-left, or back-right?
        </div>
      </div>

      <!-- Capability 3: Scene/Object Mapping -->
      <div data-step="2" class="card cgreen" style="padding:14px 18px;">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
          <div class="pstep-num green">3</div>
          <span style="font-size:14px; font-weight:600; color:var(--c-text);">Scene &amp; Object Mapping</span>
          <span style="font-size:11px; color:var(--c-text-3); margin-left:auto;">Sound source localization, reference frame construction</span>
        </div>
        <div style="background:var(--c-surface-dim); border-radius:5px; padding:10px 14px; font-size:12.5px; line-height:1.6; color:var(--c-text-2);">
          <span style="font-family:var(--f-mono); font-size:10px; font-weight:600; color:var(--c-green); letter-spacing:0.04em;">EXAMPLE</span><br>
          <strong style="color:var(--c-text);">Q:</strong> …where is <span style="background:rgba(26,122,74,0.15); padding:1px 4px; border-radius:3px; color:var(--c-green); font-weight:600;">the other person</span>? → <strong style="color:var(--c-text);">A: <span style="background:rgba(26,122,74,0.15); padding:2px 6px; border-radius:3px; color:var(--c-green);">back-right</span></strong>
        </div>
      </div>

    </div>
  </div>
  <div class="slide-footer"><span>01 — Context</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 4: STATE OF THE ART ════════════════════ -->
<div class="slide" data-section="Context" data-steps="2" id="slide-sota">
  <div class="slide-inner" style="justify-content:center;">
    <div class="stag orange">Motivation</div>
    <div class="sh sh-lg">State of the Art</div>

    <table class="tbl sota-tbl" style="margin-top:16px; width:100%; max-width:860px; font-size:14.5px;">
      <thead>
        <tr><th>Model</th><th style="text-align:center">Type</th><th>Ego-Dir</th><th>Ego-Dist</th><th>Allo-Dir</th><th>Allo-Dist</th><th>Avg</th></tr>
      </thead>
      <tbody>
        <tr class="dim"><td>Chance</td><td style="text-align:center; font-size:11px;">—</td><td>30.2</td><td>—</td><td>32.1</td><td>—</td><td>—</td></tr>
        <tr data-sota="open" class="dim"><td>VideoLLaMA2</td><td style="text-align:center"><span class="tag" style="font-size:10px;">open</span></td><td>45.8</td><td>36.3</td><td>25.9</td><td>20.4</td><td>32.1</td></tr>
        <tr data-sota="open" class="dim"><td>Ola</td><td style="text-align:center"><span class="tag" style="font-size:10px;">open</span></td><td>41.9</td><td>33.0</td><td>27.9</td><td>25.9</td><td>32.2</td></tr>
        <tr data-sota="open" class="dim"><td>MiniCPM</td><td style="text-align:center"><span class="tag" style="font-size:10px;">open</span></td><td>46.0</td><td>45.0</td><td>25.4</td><td>14.9</td><td>32.8</td></tr>
        <tr data-sota="open" class="dim"><td>EgoGPT</td><td style="text-align:center"><span class="tag" style="font-size:10px;">open</span></td><td>40.2</td><td>50.6</td><td>26.4</td><td>20.2</td><td>34.4</td></tr>
        <tr data-sota="open" class="dim"><td>Qwen2.5-Omni</td><td style="text-align:center"><span class="tag" style="font-size:10px;">open</span></td><td>55.7</td><td>25.9</td><td>31.0</td><td>5.0</td><td>29.4</td></tr>
        <tr data-sota="closed"><td>Gemini-2.5-flash</td><td style="text-align:center"><span class="tag purple" style="font-size:10px;">closed</span></td><td>74.2</td><td>49.7</td><td>29.8</td><td>29.0</td><td>45.7</td></tr>
        <tr data-sota="closed"><td>Gemini-2.5-pro</td><td style="text-align:center"><span class="tag purple" style="font-size:10px;">closed</span></td><td>75.2</td><td>59.6</td><td>31.7</td><td>37.0</td><td>50.9</td></tr>
        <tr data-sota="qwen3" class="hlb"><td><strong>Qwen3-Omni</strong></td><td style="text-align:center"><span class="tag blue" style="font-size:10px;">open</span></td><td>69.5</td><td>64.6</td><td class="bad">27.1</td><td>31.7</td><td>48.2</td></tr>
        <tr data-sota="savvy" class="hl"><td><strong>SAVVY (pipeline)</strong></td><td style="text-align:center"><span class="tag orange" style="font-size:10px;">pipeline</span></td><td class="best">84.7</td><td class="best">62.9</td><td class="best">44.0</td><td class="best">40.2</td><td class="best">58.0</td></tr>
      </tbody>
    </table>

    <div class="sota-caption" style="margin-top:16px; max-width:860px; width:100%; min-height:40px;">
      <div data-step="0" class="callout" style="font-size:12.5px; background:var(--c-surface-dim); border-color:var(--c-border);">
        Open-source AV-LLMs cluster around <strong>29–35% avg</strong>, well below closed-source models at <strong>46–51%</strong>.
      </div>
      <div data-step="1" class="callout green" style="font-size:12.5px;">
        <strong>Gap 1 — Open vs. Closed-source:</strong> Qwen3-Omni (48.2%) bridges this gap as the first open-weight model competitive with Gemini-2.5-pro (50.9%).
      </div>
      <div data-step="2" class="callout orange" style="font-size:12.5px;">
        <strong>Gap 2 — End-to-End vs. Pipeline:</strong> SAVVY adds +7.1 avg over Gemini-2.5-pro with <em>zero training</em> — structured spatial reasoning still outperforms all end-to-end models.
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>01 — Context</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 5: SECTION — SAVVY PIPELINE ════════════════════ -->
<div class="slide" data-section="SAVVY Pipeline">
  <div class="slide-inner">
    <div class="stag orange">Architecture</div>
    <div class="sh sh-lg">The SAVVY Pipeline</div>
    <div style="flex:1; display:flex; align-items:center; justify-content:center; min-height:0; margin-top:16px;">
      <div class="chart-frame" style="padding:2px; display:inline-block; line-height:0; max-width:85%;">
        <img src="assets/savvy_paper_figs/savvy_method_figure.png" alt="SAVVY method overview" style="max-height:100%; max-width:100%; height:auto; width:auto; object-fit:contain; border-radius:3px;">
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>02 — SAVVY Pipeline</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 6: PIPELINE DETAILS ════════════════════ -->
<div class="slide" data-section="SAVVY Pipeline">
  <div class="slide-inner">
    <div class="stag orange">Architecture</div>
    <div class="sh sh-lg">Two Stages of <em>Structured Spatial Reasoning</em></div>

    <div class="col-2" style="flex:1 1 auto;">
      <!-- Stage 1 -->
      <div class="card accent col gap-8" style="padding:18px; align-self:start;">
        <div style="display:flex; align-items:center; gap:8px;">
          <span class="tag orange">Stage 1</span>
          <span style="font-size:13px; font-weight:600; color:var(--c-text);">Egocentric Spatial Track Estimation</span>
        </div>
        <p style="font-size:11.5px; color:var(--c-text-3); line-height:1.5;">Three complementary extractors produce per-frame <strong>(t, θ, r)</strong> tuples — timestamp, azimuth, distance</p>
        <div class="pstep">
          <div class="pstep-num orange">a</div>
          <div class="pstep-body"><h4>Snapshot Descriptor</h4><p>One AV-LLM call grounds the query temporally, identifies object roles (target, reference, facing), and <strong>directly generates</strong> a sparse egocentric trajectory as (t, θ, r) tuples via structured prompting. Handles static objects well but misses intermediate frames.</p></div>
        </div>
        <div class="pstep">
          <div class="pstep-num blue">b</div>
          <div class="pstep-body"><h4>Text-Guided Segmentation</h4><p>CLIPSeg + SAM2 segment objects across 128 frames using SD text descriptions. θ is computed from the mask centroid's offset relative to image center; r from ZoeDepth monocular depth. Fills trajectory gaps left by the Snapshot Descriptor.</p></div>
        </div>
        <div class="pstep">
          <div class="pstep-num green">c</div>
          <div class="pstep-body"><h4>Spatial Audio Cues</h4><p>SRP-PHAT on 7-mic array estimates θ (direction-of-arrival). CDR estimates r, calibrated using visual distances from (a)/(b) via a constant D²·CDR relationship. <strong>Only modality that localizes behind-camera targets.</strong></p></div>
        </div>
      </div>

      <!-- Stage 2 -->
      <div class="card cblue" style="padding:18px; align-self:start;">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:8px;">
          <span class="tag blue">Stage 2</span>
          <span style="font-size:13px; font-weight:600; color:var(--c-text);">Dynamic Global Map Construction</span>
        </div>
        <div class="pstep">
          <div class="pstep-num blue">1</div>
          <div class="pstep-body"><h4>Ego → Global Projection</h4><p>Each (t, θ, r) track is projected onto a shared 2D map using the SLAM-derived camera pose L(t), converting egocentric observations into global coordinates.</p></div>
        </div>
        <div class="pstep">
          <div class="pstep-num blue">2</div>
          <div class="pstep-body"><h4>Track Aggregation</h4><p><strong>Static objects:</strong> DBSCAN clusters globalized positions, centroid of dominant cluster → final location. <strong>Dynamic targets:</strong> Kalman filter interpolates and smooths the trajectory across modalities.</p></div>
        </div>
        <div class="pstep">
          <div class="pstep-num blue">3</div>
          <div class="pstep-body"><h4>Spatial QA Resolution</h4><p><strong>Ego:</strong> compute direction/distance from camera to target. <strong>Allo:</strong> rotate map so reference→facing = +y axis, then read the target's relative position.</p></div>
        </div>
      </div>
    </div>

    <div class="callout green" style="font-size:12px; flex-shrink:0;">
      <strong>Ablation result:</strong> Global mapping gives <strong>+11.9%</strong> localization, <strong>+14.3%</strong> distance, <strong>+4.1%</strong> direction over raw egocentric tracks.
    </div>
  </div>
  <div class="slide-footer"><span>02 — SAVVY Pipeline</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 7: SECTION — ANALYSIS ════════════════════ -->
<div class="slide" data-section="Failure Analysis">
  <div class="divider-layout" style="gap:20px;">
    <div class="divider-num">02</div>
    <div class="sh sh-xl">Qwen3-Omni<br><em>Failure Mode Analysis</em></div>
    <p class="sp" style="max-width:520px;">What does the model get right, where does it break, and what does each failure pattern tell us about missing capabilities?</p>

    <table class="tbl" style="width:100%; max-width:680px; font-size:11.5px; margin-top:4px; opacity:0.92;">
      <thead>
        <tr><th>Model</th><th>Type</th><th>Ego-Dir</th><th>Ego-Dist</th><th>Allo-Dir</th><th>Allo-Dist</th><th>Avg</th></tr>
      </thead>
      <tbody>
        <tr class="dim"><td>Chance</td><td style="font-size:9px;">—</td><td>30.2</td><td>—</td><td>32.1</td><td>—</td><td>—</td></tr>
        <tr class="dim"><td>Open-source best (EgoGPT)</td><td><span class="tag" style="font-size:8px;">open</span></td><td>40.2</td><td>50.6</td><td>26.4</td><td>20.2</td><td>34.4</td></tr>
        <tr class="dim"><td>Gemini-2.5-pro</td><td><span class="tag" style="font-size:8px;">closed</span></td><td>75.2</td><td>59.6</td><td>31.7</td><td>37.0</td><td>50.9</td></tr>
        <tr class="hlb"><td><strong>Qwen3-Omni</strong></td><td><span class="tag blue" style="font-size:8px;">open</span></td><td>69.5</td><td>64.6</td><td class="bad">27.1</td><td>31.7</td><td>48.2</td></tr>
        <tr class="dim"><td>SAVVY (pipeline)</td><td><span class="tag" style="font-size:8px;">pipeline</span></td><td>84.7</td><td>62.9</td><td>44.0</td><td>40.2</td><td>58.0</td></tr>
      </tbody>
    </table>
  </div>
  <div class="slide-footer"><span>03 — Failure Analysis</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 8: EGO DIRECTION ════════════════════ -->
<div class="slide" data-section="Failure Analysis">
  <div class="slide-inner">
    <div class="stag red" style="color:var(--c-red);">Failure Mode 1</div>
    <div class="sh sh-lg">Ego Direction: <em>The Front Bias</em></div>

    <div style="display:flex; flex:1; align-items:center; justify-content:center;">
      <svg viewBox="0 0 340 380" height="100%" preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg" style="font-family:'Outfit',system-ui,sans-serif; max-height:420px;">
        <defs>
          <marker id="arrowRed" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="7" markerHeight="7" orient="auto-start-reverse">
            <path d="M 0 0 L 10 5 L 0 10 z" fill="#b83232"/>
          </marker>
          <linearGradient id="fovGrad" x1="0" y1="0" x2="0" y2="1">
            <stop offset="0%" stop-color="#2663b0" stop-opacity="0.10"/>
            <stop offset="100%" stop-color="#2663b0" stop-opacity="0.02"/>
          </linearGradient>
        </defs>

        <rect x="14" y="14" width="146" height="130" rx="8" fill="#e8f5ee" stroke="#1a7a4a" stroke-width="1.2" stroke-opacity="0.25"/>
        <rect x="180" y="14" width="146" height="130" rx="8" fill="#e8f5ee" stroke="#1a7a4a" stroke-width="1.2" stroke-opacity="0.25"/>
        <rect x="14" y="180" width="146" height="130" rx="8" fill="#fceaea" stroke="#b83232" stroke-width="1.2" stroke-opacity="0.25"/>
        <rect x="180" y="180" width="146" height="130" rx="8" fill="#fceaea" stroke="#b83232" stroke-width="1.2" stroke-opacity="0.25"/>

        <path d="M 170 155 L 60 18 L 280 18 Z" fill="#2663b0" opacity="0.06"/>
        <path d="M 170 155 L 60 18 L 280 18 Z" fill="none" stroke="#2663b0" stroke-width="0.75" stroke-opacity="0.2" stroke-dasharray="4 3"/>

        <text x="87" y="40" text-anchor="middle" font-size="10.5" font-weight="500" fill="#1a7a4a" opacity="0.7">Front-Left</text>
        <text x="253" y="40" text-anchor="middle" font-size="10.5" font-weight="500" fill="#1a7a4a" opacity="0.7">Front-Right</text>
        <text x="87" y="206" text-anchor="middle" font-size="10.5" font-weight="500" fill="#b83232" opacity="0.7">Back-Left</text>
        <text x="253" y="206" text-anchor="middle" font-size="10.5" font-weight="500" fill="#b83232" opacity="0.7">Back-Right</text>

        <text x="87" y="82" text-anchor="middle" font-size="38" font-weight="700" fill="#1a7a4a" font-family="'IBM Plex Mono',monospace">89%</text>
        <text x="253" y="82" text-anchor="middle" font-size="38" font-weight="700" fill="#1a7a4a" font-family="'IBM Plex Mono',monospace">74%</text>
        <text x="87" y="248" text-anchor="middle" font-size="38" font-weight="700" fill="#b83232" font-family="'IBM Plex Mono',monospace">12%</text>
        <text x="253" y="248" text-anchor="middle" font-size="38" font-weight="700" fill="#b83232" font-family="'IBM Plex Mono',monospace">7%</text>

        <text x="87" y="102" text-anchor="middle" font-size="9.5" fill="#7a7a84" font-family="'IBM Plex Mono',monospace">n=194</text>
        <text x="253" y="102" text-anchor="middle" font-size="9.5" fill="#7a7a84" font-family="'IBM Plex Mono',monospace">n=192</text>
        <text x="87" y="268" text-anchor="middle" font-size="9.5" fill="#7a7a84" font-family="'IBM Plex Mono',monospace">n=49</text>
        <text x="253" y="268" text-anchor="middle" font-size="9.5" fill="#7a7a84" font-family="'IBM Plex Mono',monospace">n=28</text>

        <path d="M 55 180 C 40 162, 40 158, 55 144" fill="none" stroke="#b83232" stroke-width="1.8" stroke-dasharray="4 2.5" marker-end="url(#arrowRed)"/>
        <text x="26" y="165" text-anchor="middle" font-size="9" font-weight="700" fill="#b83232">47%</text>
        <path d="M 285 180 C 300 162, 300 158, 285 144" fill="none" stroke="#b83232" stroke-width="1.8" stroke-dasharray="4 2.5" marker-end="url(#arrowRed)"/>
        <text x="314" y="165" text-anchor="middle" font-size="9" font-weight="700" fill="#b83232">46%</text>

        <ellipse cx="170" cy="160" rx="17" ry="21" fill="#f6f4f0" stroke="#1a1a1e" stroke-width="1.5" stroke-opacity="0.35"/>
        <path d="M 170 138 L 164 147 L 176 147 Z" fill="#1a1a1e" opacity="0.25"/>

        <text x="170" y="127" text-anchor="middle" font-size="8.5" font-weight="700" letter-spacing="0.14em" fill="#4a4a52">FRONT</text>
        <text x="170" y="198" text-anchor="middle" font-size="8.5" font-weight="700" letter-spacing="0.14em" fill="#4a4a52">BACK</text>
        <text x="6" y="162" text-anchor="middle" font-size="8.5" font-weight="700" letter-spacing="0.1em" fill="#a5a5ad" transform="rotate(-90 6 162)">LEFT</text>
        <text x="334" y="162" text-anchor="middle" font-size="8.5" font-weight="700" letter-spacing="0.1em" fill="#a5a5ad" transform="rotate(90 334 162)">RIGHT</text>

        <text x="170" y="8" text-anchor="middle" font-size="8" font-weight="500" fill="#2663b0" opacity="0.6" font-family="'IBM Plex Mono',monospace">visual field of view</text>

        <text x="170" y="340" text-anchor="middle" font-size="10" fill="#7a7a84">Per-option accuracy — Ego Direction (Qwen3-Omni)</text>
        <text x="170" y="358" text-anchor="middle" font-size="9.5" fill="#b83232" font-weight="600">Only 2 "back-right" predictions across all 463 samples</text>
      </svg>
    </div>
  </div>
  <div class="slide-footer"><span>03 — Failure Analysis · Ego Direction</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 9: CONFUSION MATRICES ════════════════════ -->
<div class="slide" data-section="Failure Analysis">
  <div class="slide-inner">
    <div class="stag red" style="color:var(--c-red);">Failure Modes 1 &amp; 2</div>
    <div class="sh sh-lg">Confusion Matrices: <em>Direction Predictions</em></div>

    <div style="display:flex; flex:1; align-items:center; justify-content:center;">
      <div class="chart-frame" style="padding:2px; display:inline-block; line-height:0; max-width:85%;">
        <img src="assets/qwen3_results/savvy_confusion_matrices.png" alt="Confusion matrices">
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>03 — Failure Analysis · Confusion Matrices</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 10: DISTANCE ════════════════════ -->
<div class="slide" data-section="Failure Analysis" data-steps="2">
  <div class="slide-inner">
    <div class="stag red" style="color:var(--c-red);">Failure Mode 3</div>
    <div class="sh sh-lg">Distance Estimation: <em>Regression Collapse</em></div>

    <div class="col-2 flex-1">
      <div class="chart-frame" style="min-height:0; overflow:hidden;">
        <img src="assets/qwen3_results/savvy_distance_scatter.png" alt="Distance scatter">
      </div>
      <div class="col gap-10">
        <div data-step="0" class="card soft" style="font-size:12px; background:var(--c-red-light);">
          <div class="card-title">Ego-Distance: Constant ~1m Output</div>
          <p>Model's answers are clustered around 0.8-1.5. MRA of 0.646 is misleading, for indoor scenes, even a constant can score well (e.g. = 1). </p>
        </div>
        <div data-step="1" class="card soft" style="font-size:12px; background:var(--c-accent-light);">
          <div class="card-title">Allo-Distance: Quantized Outputs</div>
          <p>Predictions cluster at integer values (~1/2/3/4m). Faint positive correlation for GT 2–5m, but hard ceiling at ~8m where GT extends to 12m. <strong>Some signal exists</strong> but far from calibrated.</p>
        </div>
        <div data-step="2" class="callout blue mt-auto" style="font-size:11.5px;">
          <strong>In the case of SAVVY →</strong> ZoeDepth provides calibrated per-frame metric depth. CDR provides audio-based distance. Both give actual <strong>metric</strong> signals.
        </div>
      </div>
    </div>
    <div style="align-self:flex-end; background:#f0eeea; border:1px solid #d5d0c8; border-radius:6px; padding:5px 10px; font-family:var(--f-mono); font-size:10.5px; color:var(--c-text-2); line-height:1.5;">
      MRA = <sup>1</sup>/<sub>10</sub> <span style="font-size:13px;">∑</span><sub style="font-size:8px;"><i>t</i>=0.1</sub><sup style="font-size:8px;">1.0</sup> 𝟙[ |<i>ŷ</i> − <i>y</i>| ≤ <i>t</i> ] &nbsp;<span style="color:#999; font-size:9.5px;">thresholds: 0.1m … 1.0m</span>
    </div>
  </div>
  <div class="slide-footer"><span>03 — Failure Analysis · Distance</span><span></span></div>
  <div class="progress-bar"></div> 
</div>

<!-- ════════════════════ SLIDE 11: DISTANCE DISTRIBUTIONS ════════════════════ -->
<div class="slide" data-section="Failure Analysis">
  <div class="slide-inner">
    <div class="stag red" style="color:var(--c-red);">Failure Mode 3 — Detail</div>
    <div class="sh sh-lg">Distance Distributions: <em>GT vs. Predictions</em></div>

    <div class="col-7-5 flex-1">
      <div class="chart-frame" style="min-height:0; overflow:hidden;">
        <img src="assets/qwen3_results/savvy_distance_distribution.png" alt="Distance distributions">
      </div>
      <div class="col gap-12">
        <!-- <div class="card" style="font-size:12px;">
          <div class="card-title">Distribution Mismatch</div>
          <p><strong>Ego-distance:</strong> Massive single spike at ~1m vs. GT spread 0.3–7m. The model ignores variance entirely.</p>
          <p style="margin-top:5px;"><strong>Allo-distance:</strong> Wider prediction range but over-concentrated at 1–3m vs. GT extending to 12m.</p>
        </div> -->
        <div class="callout red" style="font-size:12px;">
          <strong>Is MRA masking failure?</strong> We want to explore other metrics (regression-based) that captures the model's performance better. We believe MRA is too permissive.
        </div>
        <!-- <div class="insight-row">
          <div class="insight-tag orange">IMPLICATION</div>
          <p style="font-size:12px;"><strong>Zero metric depth calibration.</strong> AV-LLM training never requires calibrated distance outputs — the capacity may exist but supervision doesn't.</p>
        </div> -->
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>03 — Failure Analysis · Distance Distributions</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 12: MODALITY ════════════════════ -->
<div class="slide" data-section="Failure Analysis">
  <div class="slide-inner">
    <div class="stag red" style="color:var(--c-red);">Failure Mode 4</div>
    <div class="sh sh-lg">The Modality Gap: <em>Speech vs. Sound</em></div>

    <div class="col-2 flex-1">
      <div class="chart-frame" style="min-height:0; overflow:hidden;">
        <img src="assets/qwen3_results/savvy_modality_performance.png" alt="Modality performance">
      </div>
      <div class="col gap-10">
        <table class="tbl" style="font-size:12.5px;">
          <thead><tr><th>Metric</th><th>Speech</th><th>Sound</th><th>Δ</th></tr></thead>
          <tbody>
            <tr><td>Ego-Dir</td><td class="best">70.5% <span style="font-size:10px;color:var(--c-text-4);">(n=454)</span></td><td class="bad">22.2% <span style="font-size:10px;color:var(--c-text-4);">(n=9)</span></td><td class="bad">−48.3</td></tr>
            <tr><td>Ego-Dist</td><td class="best">0.654 <span style="font-size:10px;color:var(--c-text-4);">(n=169)</span></td><td class="bad">0.475 <span style="font-size:10px;color:var(--c-text-4);">(n=8)</span></td><td class="bad">−0.18</td></tr>
            <tr><td>Allo-Dir</td><td>26.3% <span style="font-size:10px;color:var(--c-text-4);">(n=529)</span></td><td>32.9% <span style="font-size:10px;color:var(--c-text-4);">(n=73)</span></td><td>+6.6</td></tr>
            <tr><td>Allo-Dist</td><td class="best">0.346 <span style="font-size:10px;color:var(--c-text-4);">(n=247)</span></td><td class="bad">0.103 <span style="font-size:10px;color:var(--c-text-4);">(n=33)</span></td><td class="bad">−0.24</td></tr>
          </tbody>
        </table>
        <!-- <div class="card cred" style="font-size:12px;">
          <div class="card-title">Transcript-Mediated "Reasoning"</div>
          <p>For speech events, the model matches described topics to recognized transcripts → finds timestamp → uses visual context. For non-speech sounds, <strong>this anchor is absent</strong> and it guesses.</p>
        </div> -->
        <!-- <div class="insight-row mt-auto">
          <div class="insight-tag red">KEY FINDING</div>
          <p style="font-size:12px;">Spatial "reasoning" is <strong>mediated entirely by language understanding</strong>, not by audio-spatial processing.</p>
        </div> -->
        <!-- <div class="callout blue" style="font-size:11.5px;">
          <strong>SAVVY fix →</strong> SRP-PHAT/CDR work on raw audio signal, not transcripts. They localize any sound — speech or environmental.
        </div> -->
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>03 — Failure Analysis · Modality</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 13: KEY WEAKNESSES ════════════════════ -->
<div class="slide" data-section="Failure Analysis" data-steps="4">
  <div class="slide-inner">
    <div class="stag red" style="color:var(--c-red);">Quick Summary</div>
    <div class="sh sh-lg">Qwen3-Omni's Key Weaknesses</div>

    <div style="display:grid; grid-template-columns:1fr 1fr; gap:16px; flex:1; align-content:start; margin-top:8px;">

      <div data-step="0" class="card soft" style="padding:14px 16px; background:var(--c-red-light);">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
          <span class="tag red" style="font-size:9px;">Slide 8</span>
          <span style="font-weight:700; font-size:14px;">Front Bias</span>
        </div>
        <p style="font-size:12px; margin:0;">The model defaults to <strong>"front"</strong> for any sound it cannot localize visually. Behind-camera directions are virtually never predicted.</p>
        <div style="margin-top:8px; font-size:11px; color:var(--c-red); font-weight:600;">73% of ego-direction predictions → front hemisphere</div>
      </div>

      <div data-step="1" class="card soft" style="padding:14px 16px; background:var(--c-red-light);">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
          <span class="tag red" style="font-size:9px;">Slide 9</span>
          <span style="font-weight:700; font-size:14px;">Allocentric Direction Collapse</span>
        </div>
        <p style="font-size:12px; margin:0;">Cannot perform coordinate-frame rotation from ego to allocentric perspective. Allocentric direction accuracy is <strong>near random</strong>.</p>
        <div style="margin-top:8px; font-size:11px; color:var(--c-red); font-weight:600;">26% accuracy on allo-direction (chance ≈ 25%)</div>
      </div>

      <div data-step="2" class="card soft" style="padding:14px 16px; background:var(--c-red-light);">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
          <span class="tag red" style="font-size:9px;">Slides 10–11</span>
          <span style="font-weight:700; font-size:14px;">Distance Regression Collapse</span>
        </div>
        <p style="font-size:12px; margin:0;">Predicts a <strong>fixed constant</strong> (~1m ego, ~2m allo) regardless of true distance. No metric depth estimation capability.</p>
        <div style="margin-top:8px; font-size:11px; color:var(--c-red); font-weight:600;">Predictions cluster at mean — zero correlation with GT</div>
      </div>

      <div data-step="3" class="card" style="padding:14px 16px; border-left-color:#aaa; opacity:0.5;">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
          <span class="tag" style="font-size:9px; background:#ddd; color:#888;">Slide 12</span>
          <span style="font-weight:700; font-size:14px; color:#999;">Modality Gap</span>
          <span style="font-size:10px; color:#999; font-style:italic; margin-left:auto;">small sample size — may not generalize</span>
        </div>
        <p style="font-size:12px; margin:0; color:#999;">Spatial reasoning is <strong>mediated entirely by speech transcripts</strong>. Non-speech sounds (claps, footsteps) have no grounding anchor.</p>
        <div style="margin-top:8px; font-size:11px; color:#999; font-weight:600;">Ego-dir: 70.5% (speech) vs 22.2% (sound)</div>
      </div>

    </div>

    <div data-step="4" class="callout red" style="font-size:12px; margin-top:auto;">
      <strong>Bottom line —</strong> Qwen3-Omni has no spatial audio processing, no metric depth, and no coordinate transforms.
    </div>
  </div>
  <div class="slide-footer"><span>03 — Failure Analysis · Key Weaknesses</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 14: SECTION — PRELIMINARY EXPERIMENT ════════════════════ -->
<div class="slide" data-section="Preliminary Experiment">
  <div class="divider-layout">
    <div class="divider-num">03</div>
    <div class="sh sh-xl">Preliminary Experiment</div>
    <p class="sp" style="max-width:540px;">Can cognitive maps help AV-LLMs reason about space?</p>
  </div>
  <div class="slide-footer"><span>03 — Preliminary Experiment</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 15: THINKING IN SPACE ════════════════════ -->
<div class="slide" data-section="Preliminary Experiment">
  <div class="slide-inner">
    <div class="stag green">Inspiration</div>
    <div class="sh sh-lg">Thinking in Space: <em>Cognitive Maps</em></div>

    <p style="font-size:10px; color:var(--c-text-4); font-family:var(--f-mono); margin-bottom:8px;">Jia et al. — "Thinking in Space" (CVPR 2025) · VSI-Bench</p>

    <!-- Top row: image (left) + cognitive maps table (right), vertically centered -->
    <div style="display:flex; gap:24px; align-items:center; flex:1; min-height:0;">
      <div class="chart-frame" style="line-height:0; overflow:hidden; flex:1; min-width:0;">
        <img src="assets/thinking_in_space.png" alt="Thinking in Space — cognitive map teaser" style="width:75%; border-radius:3px;">
      </div>
      <div class="card soft" style="padding:14px 16px; border-radius:6px; flex:0 0 auto; background:var(--c-green-light);">
        <div class="card-title" style="color:var(--c-green);">Cognitive maps help (VSI-Bench)</div>
        <table class="tbl" style="font-size:12px; margin-top:4px;">
          <thead><tr><th>Condition</th><th>Rel. Dist</th><th>Δ</th></tr></thead>
          <tbody>
            <tr><td>Baseline</td><td>46.0%</td><td>—</td></tr>
            <tr><td><strong>+ Predicted map</strong></td><td class="best">56.0%</td><td class="best">+10%</td></tr>
            <tr><td><strong>+ GT map</strong></td><td class="best">66.0%</td><td class="best">+20%</td></tr>
          </tbody>
        </table>
      </div>
    </div>

    <!-- Bottom: full-width cards -->
    <div class="card soft" style="padding:10px 14px; border-radius:6px; margin-top:10px; background:var(--c-red-light);">
      <div class="card-title" style="color:var(--c-red);">Standard prompting fails</div>
      <p style="font-size:12px; line-height:1.55;">CoT, Self-Consistency, and Tree-of-Thoughts all <strong>hurt</strong> spatial reasoning. Linguistic reasoning doesn't transfer to spatial tasks.</p>
    </div>
    <div class="callout green" style="font-size:12px; border-radius:6px; margin-top:8px;">
      <strong>Our idea →</strong> If <em>generating</em> cognitive maps helps, what if we <strong>provide pre-computed maps as input</strong> to Qwen2.5-Omni on SAVVY-Bench?
    </div>
  </div>
  <div class="slide-footer"><span>03 — Preliminary Experiment · Inspiration</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 16: OUR COGNITIVE MAPS ════════════════════ -->
<div class="slide" data-section="Preliminary Experiment">
  <div class="slide-inner">
    <div class="stag green">Results</div>
    <div class="sh sh-lg">Cognitive Maps on <em>SAVVY-Bench</em></div>

    <div class="col-7-5 flex-1" style="align-items:start;">
      <!-- Left: our cognitive maps carousel + JSON -->
      <div class="col gap-6">
        <p class="sp" style="max-width:none;">We collected top-down cognitive maps for a subset of SAVVY-Bench videos and provided them alongside the video input to Qwen3-Omni.</p>

        <!-- Carousel -->
        <div class="map-carousel" id="mapCarousel">
          <div class="carousel-track chart-frame">
            <img src="assets/loc3_script2_seq5_rec2.jpg" class="active" alt="loc3_script2_seq5_rec2" data-label="loc3_script2_seq5_rec2">
            <img src="assets/loc3_script3_seq4_rec1.jpg" alt="loc3_script3_seq4_rec1" data-label="loc3_script3_seq4_rec1">
            <img src="assets/loc3_script2_seq7_rec1.jpg" alt="loc3_script2_seq7_rec1" data-label="loc3_script2_seq7_rec1">
          </div>
          <div class="carousel-nav">
            <button class="carousel-btn" onclick="carouselNav(-1)">&#8249;</button>
            <div class="carousel-dots">
              <span class="carousel-dot active" onclick="carouselGo(0)"></span>
              <span class="carousel-dot" onclick="carouselGo(1)"></span>
              <span class="carousel-dot" onclick="carouselGo(2)"></span>
            </div>
            <button class="carousel-btn" onclick="carouselNav(1)">&#8250;</button>
            <span class="carousel-label" id="carouselLabel">loc3_script2_seq5_rec2</span>
          </div>
        </div>

        <!-- JSON structure preview -->
        <div>
          <p style="font-size:10px; font-weight:600; color:var(--c-text-3); margin-bottom:4px; font-family:var(--f-mono);">JSON structure (Based on the paper)</p>
          <div class="json-preview">[{
  <span class="jk">"video_id"</span><span class="jp">:</span> <span class="js">"loc1_script2_seq1_rec1"</span><span class="jp">,</span>
  <span class="jk">"objects"</span><span class="jp">:</span> {
    <span class="jk">"cabinet"</span><span class="jp">:</span> [[<span class="jn">0</span>,<span class="jn">1</span>], [<span class="jn">1</span>,<span class="jn">1</span>]]<span class="jp">,</span>
    <span class="jk">"tv"</span><span class="jp">:</span> [[<span class="jn">9</span>,<span class="jn">1</span>], [<span class="jn">9</span>,<span class="jn">2</span>]]<span class="jp">,</span>
    <span class="jk">"couch"</span><span class="jp">:</span> [[<span class="jn">0</span>,<span class="jn">3</span>], [<span class="jn">0</span>,<span class="jn">4</span>], [<span class="jn">0</span>,<span class="jn">5</span>]]<span class="jp">,</span>
    <span class="jk">"coffee table"</span><span class="jp">:</span> [[<span class="jn">2</span>,<span class="jn">3</span>], [<span class="jn">3</span>,<span class="jn">3</span>], ..., [<span class="jn">4</span>,<span class="jn">5</span>]]<span class="jp">,</span>
    <span class="jk">"fireplace"</span><span class="jp">:</span> [[<span class="jn">9</span>,<span class="jn">3</span>]]<span class="jp">,</span>
    <span class="jk">"window"</span><span class="jp">:</span> [[<span class="jn">0</span>,<span class="jn">2</span>], [<span class="jn">6</span>,<span class="jn">3</span>]]<span class="jp">,</span>  <span class="jp">// ...</span>
  }
}]</div>
        </div>
      </div>
      <!-- Right: results + analysis -->
      <div class="col gap-10">
        <div class="card soft" style="padding:14px 16px; background:var(--c-red-light);">
          <div class="card-title" style="color:var(--c-red);">Performance dropped across all categories</div>
          <p style="font-size:12px; line-height:1.55;">Providing cognitive maps as additional input led to a <strong>loss in performance</strong> on every task.</p>
        </div>

        <div class="card" style="padding:14px 16px;">
          <div class="card-title">Formats explored: <strong>(We will re-run these experiments)</strong></div>
          <div style="font-size:12px; line-height:1.65;">
            <p><span class="tag red" style="font-size:9px;">no gain</span> &nbsp;<strong>As image</strong>: top-down grid rendered as an image alongside video input</p>
            <p style="margin-top:5px;"><span class="tag red" style="font-size:9px;">no gain</span> &nbsp;<strong>As JSON</strong>: structured object positions as key-value pairs in the prompt</p>
            <p style="margin-top:5px;"><span class="tag red" style="font-size:9px;">no gain</span> &nbsp;<strong>As text grid</strong>: ASCII-based task grid layout in the prompt. (Needs a dictionary)</p>
          </div>
        </div>

        <div class="card" style="padding:14px 16px;">
          <div class="card-title">Possible explanations</div>
          <div style="font-size:12px; line-height:1.65;">
            <p><strong style="color:var(--c-accent);">Dynamic scenes</strong>: SAVVY-Bench has moving cameras and sound sources. A static map may not capture the temporal dynamics needed.</p>
            <p style="margin-top:5px;"><strong style="color:var(--c-accent);">Wrong formatting</strong>: We think the JSON format was not properly structured for the model to interpret, although it works for the authors. Feeding the maps as images is too "OOD" for Qwen, based on our initial experiments.</p>
          </div>
        </div>

        <div class="callout yellow" style="font-size:12px; margin-top:auto;">
          <strong>Takeaway →</strong> Cognitive maps as <em>input</em> didn't help in any format, but the concept of spatial "CoT" prompting remains promising.
        </div>
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>03 — Preliminary Experiment · Results</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 17: OTHER DIRECTIONS ════════════════════ -->
<div class="slide" data-section="Preliminary Experiment">
  <div class="slide-inner">
    <div class="stag green">Next Steps</div>
    <div class="sh sh-lg">Other Research Directions</div>
    <p class="sp" style="margin-bottom:16px;">Two complementary directions targeting the core failure modes — <strong>not</strong> exploring generation at this stage.</p>

    <div class="col-2 flex-1" style="align-items:stretch;">
      <div class="card accent col" style="padding:18px 20px;">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
          <span class="tag orange">Direction 1</span>
          <span class="tag red" style="font-size:8px;">Fixes: Front-Bias · Distance Collapse</span>
        </div>
        <div class="sh sh-sm" style="font-size:16px;">Spatial Audio Encoder Integration</div>
        <p style="font-size:12.5px; line-height:1.6; flex:1; color:var(--c-text-2);">Incorporate a <strong>spatial audio encoder</strong> (e.g., multi-channel / binaural) into the model architecture, followed by lightweight adapter training to bridge the audio encoder to the language model.</p>
        <div style="margin-top:12px; padding-top:10px; border-top:1px solid var(--c-border-light); font-size:11.5px; line-height:1.6; color:var(--c-text-3);">
          <p><strong style="color:var(--c-text-2);">Approach:</strong> Small-scale training: freeze the LLM, train only the adapter layers to project spatial audio features into the model's embedding space.</p>
          <p style="margin-top:4px;"><strong style="color:var(--c-text-2);">Expected impact:</strong> Behind-camera localization (front-bias) and metric distance estimation.</p>
        </div>
      </div>

      <div class="card cblue col" style="padding:18px 20px;">
        <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
          <span class="tag blue">Direction 2</span>
          <span class="tag red" style="font-size:8px;">Fixes: Allocentric Collapse</span>
        </div>
        <div class="sh sh-sm" style="font-size:16px;">Allocentric Perspective Transformation</div>
        <p style="font-size:12.5px; line-height:1.6; color:var(--c-text-2);">Explore how models can perform <strong>ego-to-allocentric coordinate changes</strong>, the ability to reason about spatial relationships from a third-person reference frame.</p>

        <!-- Qwen vs Gemini allocentric comparison -->
        <table class="tbl" style="margin-top:10px; width:100%; font-size:11px;">
          <thead>
            <tr><th>Model</th><th>Allo-Dir</th><th>Allo-Dist</th></tr>
          </thead>
          <tbody>
            <tr><td>Gemini-2.5-flash</td><td>29.8</td><td>29.0</td></tr>
            <tr><td>Gemini-2.5-pro</td><td style="font-weight:700; color:var(--c-blue);">31.7</td><td style="font-weight:700; color:var(--c-blue);">37.0</td></tr>
            <tr class="hlb"><td><strong>Qwen3-Omni</strong></td><td class="bad">27.1 <span style="font-size:9px; color:var(--c-red);">(-4.6)</span></td><td>31.7 <span style="font-size:9px; color:var(--c-red);">(-5.3)</span></td></tr>
          </tbody>
        </table>
        <p style="font-size:11px; line-height:1.5; margin-top:8px; color:var(--c-text-2); font-style:italic;">It would be interesting to understand where this gap to Gemini is coming from. (specific training dataset or just model capacity?)</p>

        <div style="margin-top:10px; padding-top:10px; border-top:1px solid var(--c-border-light); font-size:11.5px; line-height:1.6; color:var(--c-text-3);">
          <p><strong style="color:var(--c-text-2);">Approaches under consideration:</strong></p>
          <p style="margin-top:2px;">— Tool-augmented inference (external geometric reasoning)</p>
          <p>— Fine-tuning on perspective-change tasks</p>
          <p>— Structured prompting with explicit coordinate frames</p>
        </div>
      </div>
    </div>

    <!-- <div class="callout" style="font-size:12px; background:var(--c-surface-dim); border-color:var(--c-border); border-radius:6px; margin-top:12px;">
      <strong>Scope note →</strong> Unlike the generation-focused research of the group, our current focus is on the <strong>understanding</strong> side — improving how models perceive and reason about space, not how they generate spatial content.
    </div> -->
  </div>
  <div class="slide-footer"><span>03 — Preliminary Experiment · Next Steps</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- ════════════════════ SLIDE 18: THANKS ════════════════════ -->
<div class="slide">
  <div class="slide-inner" style="justify-content:center; align-items:center;">
    <div class="sh" style="font-size:96px; letter-spacing:-0.03em;">Thanks!</div>
  </div>
  <div class="slide-footer"><span>Spatial Understanding with AV-LLMs · Edson Araujo</span><span></span></div>
  <div class="progress-bar"></div>
</div>

<!-- OLD SLIDE 13 (commented out)
<div class="slide" data-section="Failure Analysis">
  <div class="slide-inner">
    <div class="stag dim">Synthesis</div>
    <div class="sh sh-lg">What the Model <em>Actually Does</em></div>
    <div class="col-2 flex-1">
      <div class="codeblock" style="font-size:10.5px; line-height:1.8; flex:1;">
        ... original content preserved ...
      </div>
    </div>
  </div>
</div>
-->

<!-- ════════════════════ SLIDE 14: SECTION — RESEARCH ════════════════════ -->
<!-- <div class="slide" data-section="Research Directions">
  <div class="divider-layout">
    <div class="divider-num">03</div>
    <div class="sh sh-xl">Research Directions</div>
    <p class="sp" style="max-width:540px;">Six ideas for bridging Gap 2 — each anchored to a specific empirical failure mode from our analysis.</p>
  </div>
  <div class="slide-footer"><span>04 — Research Directions</span><span></span></div>
  <div class="progress-bar"></div>
</div> -->

<!-- ════════════════════ SLIDE 15: DIRECTIONS 1–3 ════════════════════ -->
<!-- <div class="slide" data-section="Research Directions">
  <div class="slide-inner">
    <div class="stag orange">High Priority</div>
    <div class="sh sh-lg">Directions 1–3: <em>Targeted Interventions</em></div>

    <div class="col-3 flex-1" style="align-items:stretch;">
      <div class="card accent col" style="padding:16px 18px;">
        <div style="display:flex; align-items:center; gap:6px; margin-bottom:8px;">
          <span class="tag orange">Dir 1</span>
          <span class="tag red" style="font-size:8px;">Fixes: Front-Bias</span>
        </div>
        <div class="sh sh-sm" style="font-size:15px;">Spatial Audio Adapter</div>
        <p style="font-size:11.5px; line-height:1.55; flex:1;">Train lightweight multi-channel audio encoder for DoA. Inject spatial embeddings into LLM without full retraining. Even a <strong>binary front/back signal</strong> would transform ego-direction. Train on synthetic spatial audio (RIR simulation).</p>
        <div style="margin-top:auto; padding-top:10px; border-top:1px solid var(--c-border-light);">
          <div style="font-size:10px; color:var(--c-text-3);"><strong>Effort:</strong> Medium · <strong>Impact:</strong> Ego-Dir +15–20%</div>
        </div>
      </div>

      <div class="card cblue col" style="padding:16px 18px;">
        <div style="display:flex; align-items:center; gap:6px; margin-bottom:8px;">
          <span class="tag blue">Dir 2</span>
          <span class="tag red" style="font-size:8px;">Fixes: All Tasks</span>
        </div>
        <div class="sh sh-sm" style="font-size:15px;">Structured Output Decomposition</div>
        <p style="font-size:11.5px; line-height:1.55; flex:1;">Prompt model to decompose: temporal grounding → visibility check → direction/distance. <strong>Surfaces uncertainty</strong> instead of false-confident "front" defaults. Mirrors SAVVY's Snapshot Descriptor format.</p>
        <div style="margin-top:auto; padding-top:10px; border-top:1px solid var(--c-border-light);">
          <div style="font-size:10px; color:var(--c-text-3);"><strong>Effort:</strong> Low · <strong>Impact:</strong> Diagnostic + Moderate</div>
        </div>
      </div>

      <div class="card cgreen col" style="padding:16px 18px;">
        <div style="display:flex; align-items:center; gap:6px; margin-bottom:8px;">
          <span class="tag green">Dir 3</span>
          <span class="tag red" style="font-size:8px;">Fixes: Distance Collapse</span>
        </div>
        <div class="sh sh-sm" style="font-size:15px;">Depth-Aware Training</div>
        <p style="font-size:11.5px; line-height:1.55; flex:1;">Curriculum: coarse bins (near/medium/far) → meter-level regression → calibrated estimation with ZoeDepth pseudo-labels. Allo scatter shows model <strong>can produce varied outputs</strong> — needs supervision, not capacity.</p>
        <div style="margin-top:auto; padding-top:10px; border-top:1px solid var(--c-border-light);">
          <div style="font-size:10px; color:var(--c-text-3);"><strong>Effort:</strong> Medium · <strong>Impact:</strong> Dist +15–25%</div>
        </div>
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>04 — Research Directions · High Priority</span><span></span></div>
  <div class="progress-bar"></div>
</div> -->

<!-- ════════════════════ SLIDE 16: DIRECTIONS 4–6 ════════════════════ -->
<!-- <div class="slide" data-section="Research Directions">
  <div class="slide-inner">
    <div class="stag purple">Exploratory</div>
    <div class="sh sh-lg">Directions 4–6: <em>Deeper Interventions</em></div>

    <div class="col-3 flex-1" style="align-items:stretch;">
      <div class="card cpurple col" style="padding:16px 18px;">
        <div style="display:flex; align-items:center; gap:6px; margin-bottom:8px;">
          <span class="tag purple">Dir 4</span>
          <span class="tag red" style="font-size:8px;">Fixes: Allo-Dir</span>
        </div>
        <div class="sh sh-sm" style="font-size:15px;">Cognitive Map Prompting</div>
        <p style="font-size:11.5px; line-height:1.55; flex:1;">Prompt model to generate top-down grid layout before answering allocentric questions (VSI-Bench showed +10% distance accuracy). <strong>Challenge:</strong> MLLMs build strong local but weak global maps — exactly where allo-QA is hardest.</p>
        <div style="margin-top:auto; padding-top:10px; border-top:1px solid var(--c-border-light);">
          <div style="font-size:10px; color:var(--c-text-3);"><strong>Effort:</strong> Low · <strong>Impact:</strong> Speculative</div>
        </div>
      </div>

      <div class="card accent col" style="padding:16px 18px;">
        <div style="display:flex; align-items:center; gap:6px; margin-bottom:8px;">
          <span class="tag orange">Dir 5</span>
          <span class="tag red" style="font-size:8px;">Fixes: Perception</span>
        </div>
        <div class="sh sh-sm" style="font-size:15px;">Tool-Augmented Inference</div>
        <p style="font-size:11.5px; line-height:1.55; flex:1;">Give callable tools: <span style="font-family:var(--f-mono); font-size:10.5px; color:var(--c-blue);">depth_estimate()</span>, <span style="font-family:var(--f-mono); font-size:10.5px; color:var(--c-blue);">segment()</span>, <span style="font-family:var(--f-mono); font-size:10.5px; color:var(--c-blue);">audio_doa()</span>. Unlike SAVVY (128 frames × 3 modalities), this is <strong>selective and query-driven</strong>.</p>
        <div style="margin-top:auto; padding-top:10px; border-top:1px solid var(--c-border-light);">
          <div style="font-size:10px; color:var(--c-text-3);"><strong>Effort:</strong> Medium · <strong>Impact:</strong> High (practical)</div>
        </div>
      </div>

      <div class="card cblue col" style="padding:16px 18px;">
        <div style="display:flex; align-items:center; gap:6px; margin-bottom:8px;">
          <span class="tag blue">Dir 6</span>
          <span class="tag red" style="font-size:8px;">Fixes: Full Gap</span>
        </div>
        <div class="sh sh-sm" style="font-size:15px;">SAVVY Pipeline Distillation</div>
        <p style="font-size:11.5px; line-height:1.55; flex:1;">Run SAVVY on large corpus → collect structured traces (grounding, tracks, global map) → fine-tune model to produce them as chain-of-thought. SAVVY's own stated future direction. <strong>Highest ceiling.</strong></p>
        <div style="margin-top:auto; padding-top:10px; border-top:1px solid var(--c-border-light);">
          <div style="font-size:10px; color:var(--c-text-3);"><strong>Effort:</strong> High · <strong>Impact:</strong> Highest ceiling</div>
        </div>
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>04 — Research Directions · Exploratory</span><span></span></div>
  <div class="progress-bar"></div>
</div> -->

<!-- ════════════════════ SLIDE 17: ROADMAP ════════════════════ -->
<!-- <div class="slide" data-section="Research Directions">
  <div class="slide-inner">
    <div class="stag orange">Prioritization</div>
    <div class="sh sh-lg">Research Roadmap</div>

    <div class="col-7-5 flex-1">
      <div class="col gap-16">
        <table class="tbl" style="font-size:12.5px;">
          <thead><tr><th></th><th>Direction</th><th>Effort</th><th>Impact</th><th>Linked Failure</th></tr></thead>
          <tbody>
            <tr class="hl"><td>🥇</td><td><strong>Spatial Audio Adapter</strong></td><td>Medium</td><td>High</td><td>Front-bias</td></tr>
            <tr class="hl"><td>🥈</td><td><strong>Structured Output Decomp.</strong></td><td>Low</td><td>Medium</td><td>All tasks</td></tr>
            <tr class="hl"><td>🥉</td><td><strong>Depth-Aware Training</strong></td><td>Medium</td><td>High</td><td>Regression collapse</td></tr>
            <tr><td>4</td><td>Cognitive Map Prompting</td><td>Low</td><td>Speculative</td><td>Allo-dir collapse</td></tr>
            <tr><td>5</td><td>Tool-Augmented Inference</td><td>Medium</td><td>High</td><td>All perception</td></tr>
            <tr><td>6</td><td>SAVVY Distillation</td><td>High</td><td>Highest</td><td>Full gap</td></tr>
          </tbody>
        </table>
        <div class="callout orange" style="font-size:12px;">
          <strong>Design principle:</strong> Every research direction is anchored to a specific empirical failure mode — not speculative, but data-driven.
        </div>
      </div>
      <div class="col gap-12">
        <div class="card" style="font-size:12px;">
          <div class="card-title">Open Questions</div>
          <p>• Is MRA masking distance failure? Need stricter metrics (Pearson r, per-bin accuracy).</p>
          <p style="margin-top:5px;">• How much of SAVVY's advantage comes from multi-channel audio access alone?</p>
          <p style="margin-top:5px;">• Can allocentric reasoning be tested in isolation with text-only coordinates?</p>
          <p style="margin-top:5px;">• What drove the Qwen3 vs 2.5 jump (+18.8 avg)?</p>
        </div>
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>04 — Research Directions · Roadmap</span><span></span></div>
  <div class="progress-bar"></div>
</div> -->

<!-- ════════════════════ SLIDE 18: TAKEAWAYS ════════════════════ -->
<!-- <div class="slide" data-section="Summary">
  <div class="slide-inner" style="justify-content:center; padding-top:48px;">
    <div class="stag dim" style="justify-content:center; text-align:center; display:block; margin-bottom:8px;">Summary</div>
    <div class="sh sh-xl" style="text-align:center; margin-bottom:32px; max-width:100%;">Key Takeaways</div>

    <div style="max-width:740px; margin:0 auto; display:flex; flex-direction:column; gap:18px;">
      <div style="display:flex; gap:14px; align-items:flex-start;">
        <div style="font-family:var(--f-mono); font-size:11px; font-weight:700; color:white; background:var(--c-accent); min-width:26px; height:26px; border-radius:50%; display:flex; align-items:center; justify-content:center; flex-shrink:0; margin-top:1px;">1</div>
        <p style="font-size:14px; line-height:1.6; color:var(--c-text-2);"><strong style="color:var(--c-text);">Qwen3-Omni bridges Gap 1</strong> (open vs. closed-source) but not Gap 2 (end-to-end vs. pipeline). Structured spatial reasoning remains essential.</p>
      </div>
      <div style="display:flex; gap:14px; align-items:flex-start;">
        <div style="font-family:var(--f-mono); font-size:11px; font-weight:700; color:white; background:var(--c-blue); min-width:26px; height:26px; border-radius:50%; display:flex; align-items:center; justify-content:center; flex-shrink:0; margin-top:1px;">2</div>
        <p style="font-size:14px; line-height:1.6; color:var(--c-text-2);"><strong style="color:var(--c-text);">The gap is concentrated in three specific failures:</strong> behind-camera localization (front-bias), metric distance estimation (regression collapse), and allocentric coordinate transformation (mode collapse).</p>
      </div>
      <div style="display:flex; gap:14px; align-items:flex-start;">
        <div style="font-family:var(--f-mono); font-size:11px; font-weight:700; color:white; background:var(--c-green); min-width:26px; height:26px; border-radius:50%; display:flex; align-items:center; justify-content:center; flex-shrink:0; margin-top:1px;">3</div>
        <p style="font-size:14px; line-height:1.6; color:var(--c-text-2);"><strong style="color:var(--c-text);">Each failure maps to a specific SAVVY component,</strong> suggesting targeted interventions — spatial audio adapter, depth training, geometric reasoning — rather than architectural overhaul.</p>
      </div>
      <div style="display:flex; gap:14px; align-items:flex-start;">
        <div style="font-family:var(--f-mono); font-size:11px; font-weight:700; color:white; background:var(--c-purple); min-width:26px; height:26px; border-radius:50%; display:flex; align-items:center; justify-content:center; flex-shrink:0; margin-top:1px;">4</div>
        <p style="font-size:14px; line-height:1.6; color:var(--c-text-2);"><strong style="color:var(--c-text);">The model has genuine strengths to build on:</strong> temporal grounding for speech, front-hemisphere direction, left/right discrimination. Bridge the gap by adding what's missing, not starting over.</p>
      </div>
    </div>
  </div>
  <div class="slide-footer"><span>Spatial Understanding with AV-LLMs · Edson Araujo</span><span></span></div>
  <div class="progress-bar"></div>
</div> -->

</div><!-- end deck -->

<script>
const slides = document.querySelectorAll('.slide');
const total = slides.length;
let cur = 0;

// Sub-step tracking: each slide can have data-steps="N"
// subStep[i] tracks current sub-step (0 = base, 1..N = highlights)
const subStep = new Array(total).fill(0);

function getMaxSteps(i) {
  return parseInt(slides[i].getAttribute('data-steps') || '0', 10);
}

function applySubStep(i) {
  const slide = slides[i];
  const step = subStep[i];

  // --- Intro-papers system (slide 1) ---
  const papers = slide.querySelector('.intro-papers');
  if (papers) {
    const cards = papers.querySelectorAll('.card');
    const bannerItems = slide.querySelectorAll('.intro-banner-item');
    papers.classList.remove('highlighting');
    cards.forEach(c => c.classList.remove('spot'));
    bannerItems.forEach(b => b.classList.remove('spot'));

    if (step > 0 && step < 5) {
      papers.classList.add('highlighting');
      cards.forEach(c => {
        c.classList.toggle('spot', parseInt(c.getAttribute('data-paper'), 10) === step - 1);
      });
    } else if (step === 5) {
      papers.classList.add('highlighting');
      bannerItems.forEach(b => b.classList.add('spot'));
    }
  }

  // --- Generic data-step system ---
  // step 0 highlights data-step="0", step 1 highlights data-step="1", etc.
  const stepEls = slide.querySelectorAll('[data-step]');
  if (stepEls.length > 0) {
    const inner = slide.querySelector('.slide-inner') || slide;
    inner.classList.add('step-active');
    stepEls.forEach(el => {
      el.classList.toggle('step-lit', parseInt(el.getAttribute('data-step'), 10) === step);
    });
  }

  // --- SOTA table row highlighting ---
  const sotaTbl = slide.querySelector('.sota-tbl');
  if (sotaTbl) {
    const rows = sotaTbl.querySelectorAll('tbody tr[data-sota]');
    if (step === 0) {
      // Highlight open-source (dim) vs closed-source gap
      sotaTbl.classList.add('highlighting');
      rows.forEach(r => {
        const g = r.getAttribute('data-sota');
        r.classList.toggle('sota-lit', g === 'open' || g === 'closed');
      });
    } else if (step === 1) {
      // Highlight Qwen3-Omni bridging gap 1
      sotaTbl.classList.add('highlighting');
      rows.forEach(r => {
        const g = r.getAttribute('data-sota');
        r.classList.toggle('sota-lit', g === 'qwen3' || g === 'closed');
      });
    } else if (step === 2) {
      // Highlight SAVVY vs best e2e
      sotaTbl.classList.add('highlighting');
      rows.forEach(r => {
        const g = r.getAttribute('data-sota');
        r.classList.toggle('sota-lit', g === 'savvy' || g === 'closed');
      });
    } else {
      sotaTbl.classList.remove('highlighting');
      rows.forEach(r => r.classList.remove('sota-lit'));
    }
  }
}

function go(n) {
  if (n < 0 || n >= total) return;
  slides[cur].classList.remove('active');
  // Reset sub-step of the slide we're leaving
  subStep[cur] = 0;
  applySubStep(cur);
  cur = n;
  slides[cur].classList.add('active');
  applySubStep(cur);
  // Update all counters
  document.querySelectorAll('.slide-footer').forEach((f, i) => {
    const sp = f.querySelectorAll('span');
    if (sp[1]) sp[1].textContent = (i + 1) + ' / ' + total;
  });
  // Update all progress bars
  document.querySelectorAll('.progress-bar').forEach(bar => {
    bar.style.width = ((cur + 1) / total * 100) + '%';
  });
}

function advance() {
  const max = getMaxSteps(cur);
  if (max > 0 && subStep[cur] < max) {
    subStep[cur]++;
    applySubStep(cur);
  } else {
    go(cur + 1);
  }
}

function retreat() {
  if (subStep[cur] > 0) {
    subStep[cur]--;
    applySubStep(cur);
  } else {
    go(cur - 1);
  }
}

document.addEventListener('keydown', e => {
  if (e.key === 'ArrowRight' || e.key === ' ' || e.key === 'PageDown') { e.preventDefault(); advance(); }
  if (e.key === 'ArrowLeft' || e.key === 'PageUp') { e.preventDefault(); retreat(); }
  if (e.key === 'Home') { e.preventDefault(); go(0); }
  if (e.key === 'End') { e.preventDefault(); go(total - 1); }
});

let tx = 0;
document.addEventListener('touchstart', e => { tx = e.touches[0].clientX; });
document.addEventListener('touchend', e => {
  const d = tx - e.changedTouches[0].clientX;
  if (Math.abs(d) > 50) d > 0 ? advance() : retreat();
});


function scaleDeck() {
  const deck = document.getElementById('deck');
  const sx = window.innerWidth / 1280;
  const sy = window.innerHeight / 720;
  deck.style.transform = 'scale(' + Math.min(sx, sy) + ')';
}
scaleDeck();
window.addEventListener('resize', scaleDeck);

go(0);

// Video fullscreen overlay
const overlay = document.createElement('div');
overlay.id = 'video-overlay';
overlay.innerHTML = '<video controls></video>';
document.body.appendChild(overlay);
const ovVideo = overlay.querySelector('video');

function openVideoOverlay(src) {
  ovVideo.src = src;
  overlay.classList.add('active');
  ovVideo.currentTime = 0;
  ovVideo.play();
}
function closeVideoOverlay() {
  overlay.classList.remove('active');
  ovVideo.pause();
  ovVideo.src = '';
}
overlay.addEventListener('click', function(e) {
  if (e.target === overlay) closeVideoOverlay();
});
document.addEventListener('keydown', function(e) {
  if (e.key === 'Escape' && overlay.classList.contains('active')) {
    closeVideoOverlay();
    e.stopPropagation();
  }
}, true);

// Image zoom overlay
const imgOverlay = document.createElement('div');
imgOverlay.id = 'img-overlay';
imgOverlay.innerHTML = '<img>';
document.body.appendChild(imgOverlay);
const ovImg = imgOverlay.querySelector('img');

document.querySelectorAll('.chart-frame img').forEach(function(img) {
  img.addEventListener('click', function(e) {
    e.stopPropagation();
    ovImg.src = img.src;
    imgOverlay.classList.add('active');
  });
});
imgOverlay.addEventListener('click', function() {
  imgOverlay.classList.remove('active');
  ovImg.src = '';
});
document.addEventListener('keydown', function(e) {
  if (e.key === 'Escape' && imgOverlay.classList.contains('active')) {
    imgOverlay.classList.remove('active');
    ovImg.src = '';
    e.stopPropagation();
  }
}, true);

// Slide jump dialog
const jumpEl = document.createElement('div');
jumpEl.id = 'slide-jump';
jumpEl.innerHTML = '<div id="slide-jump-box"><label>Go to slide</label><input type="text" id="slide-jump-input" maxlength="2"><div class="hint">1–' + total + ' · Enter to go · Esc to cancel</div></div>';
document.body.appendChild(jumpEl);
const jumpInput = document.getElementById('slide-jump-input');

function openJump() { jumpEl.classList.add('active'); jumpInput.value = ''; jumpInput.focus(); }
function closeJump() { jumpEl.classList.remove('active'); }

jumpEl.addEventListener('click', function(e) { if (e.target === jumpEl) closeJump(); });
jumpInput.addEventListener('keydown', function(e) {
  if (e.key === 'Escape') { closeJump(); e.stopPropagation(); }
  if (e.key === 'Enter') {
    const n = parseInt(jumpInput.value);
    if (n >= 1 && n <= total) go(n - 1);
    closeJump();
    e.stopPropagation();
  }
});

document.addEventListener('keydown', function(e) {
  if (overlay.classList.contains('active') || jumpEl.classList.contains('active')) return;
  if (e.key === 'g' || e.key === 'G') { e.preventDefault(); openJump(); }
});

// ═══ MAP CAROUSEL ═══
(function() {
  const carousel = document.getElementById('mapCarousel');
  if (!carousel) return;
  const imgs = carousel.querySelectorAll('.carousel-track img');
  const dots = carousel.querySelectorAll('.carousel-dot');
  const label = document.getElementById('carouselLabel');
  let idx = 0;
  function show(n) {
    idx = (n + imgs.length) % imgs.length;
    imgs.forEach(function(img, i) { img.classList.toggle('active', i === idx); });
    dots.forEach(function(d, i) { d.classList.toggle('active', i === idx); });
    if (label) label.textContent = imgs[idx].getAttribute('data-label');
  }
  window.carouselNav = function(dir) { show(idx + dir); };
  window.carouselGo = function(n) { show(n); };
})();
</script>
</body>
</html>
